<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>BO-QSA</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <style>
    body {
      background-color: #fafafa;
    }
    hr {
      background-color: #f1f1f1;
    }
    .table th {
      padding: 0.2em 0.2em 0 0.2em;
      width: 50%;
    }
    .table td {
      padding: 0.25em;
      width: 50%;
    }
    .content-block {
      padding: 0 6px;
    }
    .title {
      font-size: 32px;
      padding-top: 32px;
    }
    .publisher {
      margin-bottom: 1rem;
    }
    .sub-title {
      font-size: 26px;
      padding-bottom: 16px;
    }
    .subsub-title {
      font-size: 20px;
      padding-bottom: 8px;
    }
    .author {
      font-size: 16px;
    }
    .task {
      padding-bottom: 12px;
    }
  </style>
  <script type="importmap">
    {
      "imports": {
        "vue": "https://unpkg.com/vue@3/dist/vue.esm-browser.js",
        "three": "https://unpkg.com/three@0.127.0/build/three.module.js",
        "three/addons/": "https://unpkg.com/three@0.127.0/examples/jsm/"
      }
    }
  </script>
</head>

<body>
  <div id="app">
    <div class="columns">
      <div class="column" :class="[content, offset]">

        <h1 class="title has-text-centered" style="margin-bottom: 0.5em">
            Improving Unsupervised Object-centric Learning with Query Optimization
        </h1>

        <!-- <p class="publisher has-text-centered">Conference</p> -->

        <div class="has-text-centered" style="padding: 0 2em;">

          <p class="author">
            <a href="https://buzz-beater.github.io/">Baoxiong Jia*</a><sup>1,3</sup><br>
            <a href="https://yuliu.com">Yu Liu✶</a><sup>2,3</sup>,</span>
            <a href="https://siyuanhuang.com/">Siyuan Huang</a><sup>3</sup> &nbsp;&nbsp;&nbsp;
          </p>

          <p style="font-size: 0.9em; padding: 0.5em 0;">✶ indicates equal contribution</p>

          <p style="font-size: 1em;">
            <sup>1</sup>UCLA<br>
            <sup>2</sup>Tsinghua University<br>
            <sup>3</sup>Beijing Institute for General Artificial Intelligence (BIGAI)<br>
          </p>

          <div style="margin: 1em 0; font-size: 1.1em;">
            <p> <a href="https://arxiv.org/abs/2301.06015">arXiv</a> &nbsp;|&nbsp; <a href="https://github.com/scenediffuser/Scene-Diffuser">Code</a> &nbsp;|&nbsp; <a href="https://huggingface.co/spaces/SceneDiffuser/SceneDiffuserDemo">Huggingface Space</a></p>
          </div>
        </div>

        <div class="has-text-centered" style="padding: 0 1em;">
          <img src="./static/images/overview.png"/>
          <p style="text-align: justify; font-size: 0.8em;">
            We proposed methods for (1) initializing Slot-Attention modules with learnable queries and (2) optimizing 
            the model with bi-level optimization. With simple code adjustments on the 
            vanilla Slot-Attention, our model, Bi-level Optimized Query Slot Attention, 
            achieves state-of-the-art results on 3 challenging synthetic and 7 complex real-world datasets in 
            unsupervised image segmentation and reconstruction, outperforming previous baselines by a large margin.
          </p>
        </div>

        <hr>

        <div class="has-text-centered content-block">
          <h2 class="sub-title">Abstract</h2>
            <p style="text-align:justify; margin-bottom: 1.5em;">
            The ability to decompose complex natural scenes into meaningful object-centric abstractions 
            lies at the core of human perception and reasoning. In the recent culmina- tion of 
            unsupervised object-centric learning, the Slot-Attention module has played an important 
            role with its simple yet effective design and fostered many powerful variants. 
            These methods, however, have been exceedingly difficult to train without supervision and 
            are ambiguous in the notion of object, especially for complex natu- ral scenes. In this paper, 
            we propose to address these issues by improving previous attempts that leverage bi-level optimization 
            in Slot-Attention with learnable query initializations and straight-through gradient updates. 
            With simple code adjustments on Slot-Attention, our model, Bi-level Optimized Query Slot Attention, 
            achieves state-of-the-art results on 3 challenging synthetic and 7 complex real-world datasets in 
            unsupervised image segmentation and reconstruction, outperforming previous baselines by a large margin. 
            We provide thorough ablative studies to validate the necessity and effectiveness of our design. 
            Additionally, our model exhibits great potential for concept binding and zero-shot learning. 
            We hope our effort could provide a single home for the design and learning of slot-based models and 
            pave the way for more challenging tasks in object-centric learning.
            </p>
          <video controls="controls" loop="loop" autoplay="autoplay" style="padding: 0 5em;">
            <source src="./assets/illustration-720.mp4" type="video/mp4">
          </video>
        </div>

        <hr>

        <div id="results" class="content-block">
            <h2 class="sub-title has-text-centered">Results for real-world multi-object datasets</h2>
  
            <div class="task">
              <div class="columns is-centered" style="position: relative;">
                <div class="control" style="position: absolute; left: 3%;">
                  <label class="radio">
                    <input value="Birds" type="radio" name="scene"> Birds
                  </label>
                  <br>
                  <label class="radio">
                    <input value="COCO" type="radio" name="scene"> COCO
                  </label>
                  <br>
                  <label class="radio">
                    <input value="YCB" type="radio" name="scene"> YCB
                  </label>
                  <br>
                  <label class="radio">
                    <input value="ScanNet" type="radio" name="scene"> ScanNet
                  </label>
                </div>
                <div id="pose_loading" style="position: absolute; left: 50%; top: 45%;"></div>
                <img poster="" src="./static/images/birds.png" id="birds" height="100%"></img>
              </div>
              <p style="margin: 0 1.5em; font-size: 0.8em; text-align:justify;">Note: Click the radio button to select a dataset for result visualization.</p>
            </div>
          </div>

        <hr>

        <div class="has-text-centered content-block">
          <h2 class="sub-title">Bibtex</h2>
          <p style="text-align:justify">If you find our project useful, please consider citing us:</p>
          <div style="padding-top: 1rem; text-align: left;">
<pre><code>
    @article{jia2022unsupervised,
        title={Unsupervised Object-Centric Learning with Bi-Level Optimized Query Slot Attention},
        author={Jia, Baoxiong and Liu, Yu and Huang, Siyuan},
        journal={arXiv preprint arXiv:2210.08990},
        year={2022}
      }
</code></pre>
          </div>
        </div>

      </div>
    </div>
    
    <footer class="footer" style="padding: 0 0 1.5rem 0">
      <div class="columns">
        <div class="column" :class="[content, offset]">
          <hr>
          <p class="has-text-centered" style="font-size: 0.9em;">This website uses the template from <a href="https://scenediffuser.github.io">SceneDiffuser</a>. Please use <b>chrome</b> for best experience. </p>
        </div>
      </div>
    </footer>
  </div>
</body>

<script type="module">
  import { createApp } from 'vue'

  let app = createApp({
    data() {
      return {
        content: 'is-6',
        offset: 'is-offset-3'
      }
    }
  }).mount('#app')

  function displayWindowSize() {
    let w = document.documentElement.clientWidth;
    // console.log(w)
    let c
    let o
    if (w >= 1200) {
      c = 'is-6'
      o = 'is-offset-3'
    } else {
      if (w >= 900) {
        c = 'is-8'
        o = 'is-offset-2'
      } else {
        c = 'is-10'
        o = 'is-offset-1'
      }
    }
    app.$data.content = c
    app.$data.offset = o
  }
  window.addEventListener("resize", displayWindowSize)
  displayWindowSize()

  import * as THREE from 'three'
  import { OrbitControls } from 'three/addons/controls/OrbitControls.js'
  import {GLTFLoader} from 'three/addons/loaders/GLTFLoader.js'

  // pose generation in 3D scene
  let canvas1 = document.querySelector('#webgl_pose')
  let scene1 = new THREE.Scene()
  let assetLoader1 = new GLTFLoader()
  let model1

  let camera1 = new THREE.PerspectiveCamera(45, 1.618 / 1.0, 0.1, 100)
  camera1.position.set(4, 3, -3)
  let grid1 = new THREE.GridHelper(30, 30)
  scene1.add(camera1)
  scene1.add(grid1)
  for (let i = 0; i <= 1; i++) {
    for (let k = 0; k <= 1; k++) {
      let spotLight = new THREE.SpotLight(0xAAAAAA)
      spotLight.position.set(50 * (i * 2 - 1), 100, 100 * (k * 2 - 1))
      scene1.add(spotLight)
    }
  }

  let controls1 = new OrbitControls(camera1, canvas1)
  controls1.enableZoom = true
  // controls2.enableDamping = true
  controls1.object.position.set(camera1.position.x, camera1.position.y, camera1.position.z)
  controls1.target = new THREE.Vector3(0, 0, 0)
  controls1.update()

  let renderer1 = new THREE.WebGLRenderer({
      canvas: canvas1,
      alpha: true,
  })
  renderer1.setPixelRatio(Math.min(window.devicePixelRatio, 2))
  renderer1.outputEncoding = THREE.sRGBEncoding
  renderer1.setAnimationLoop(() => {
    renderer1.render(scene1, camera1)
  });

  const radioButtons = document.querySelectorAll('input[name="scene"]')
  for (const radioButton of radioButtons) {
    radioButton.addEventListener('change', (e) => {
      scene1.remove(model1)
      document.querySelector('#pose_loading').innerHTML = `<img src="./assets/icons/loading.svg" width="48" height="48">`
      for (const rb of radioButtons) {rb.disabled = true}

      let assetUrl = new URL('./assets/pose_generation/'+radioButton.value+'.glb', import.meta.url)
      assetLoader1.load(assetUrl.href, gltf => {
        model1 = gltf.scene
        model1.rotateX(-Math.PI / 2)
        scene1.add(model1)
        document.querySelector('#pose_loading').innerHTML = ''
        for (const rb of radioButtons) {rb.disabled = false}
      }, undefined, (error) => {console.error(error)})
    })
  }
  radioButtons[0].click()

  // grasp generation for 3D object
  let canvas2 = document.querySelector('#webgl_grasp')
  let scene2 = new THREE.Scene()
  let assetLoader2 = new GLTFLoader()
  let model2

  let camera2 = new THREE.PerspectiveCamera(45, 1.618 / 1.0, 0.1, 100)
  camera2.position.set(0.25, 0.2, -0.25)
  let grid2 = new THREE.GridHelper(30, 30)
  scene2.add(camera2)
  scene2.add(grid2)
  for (let ax = 0; ax < 3; ax++) {
    for (let i = 0; i <= 1; i++) {
      let spotLight = new THREE.SpotLight(0xAAAAAA)
      spotLight.position.set(
        ax == 0 ? 50 * (i * 2 - 1): 0,
        ax == 1 ? 50 * (i * 2 - 1): 0,
        ax == 2 ? 50 * (i * 2 - 1): 0,
      )
      scene2.add(spotLight)
    }
  }

  let controls2 = new OrbitControls(camera2, canvas2)
  controls2.enableZoom = true
  // controls2.enableDamping = true
  controls2.object.position.set(camera2.position.x, camera2.position.y, camera2.position.z)
  controls2.target = new THREE.Vector3(0, 0, 0)
  controls2.update()

  let renderer2 = new THREE.WebGLRenderer({
      canvas: canvas2,
      alpha: true,
  })
  renderer2.setPixelRatio(Math.min(window.devicePixelRatio, 2))
  renderer2.outputEncoding = THREE.sRGBEncoding
  renderer2.setAnimationLoop(() => {
    renderer2.render(scene2, camera2)
  });

  const objSel = document.querySelector('#objectSelector')
  const samSel = document.querySelector('#sampleSelector')
  objSel.value = 'ycb+baseball'
  samSel.value = 'grasp_0'

  function load_model2(e) {
    if (objSel.value == '' || samSel.value == '') {
      return
    }
    scene2.remove(model2)
    document.querySelector('#grasp_loading').innerHTML = `<img src="./assets/icons/loading.svg" width="48" height="48">`
    let assetUrl = new URL(`./assets/grasp_generation_color/${objSel.value}/${samSel.value}.glb`, import.meta.url)
    assetLoader2.load(assetUrl.href, gltf => {
      model2 = gltf.scene
      scene2.add(model2)
      document.querySelector('#grasp_loading').innerHTML = ''
    }, undefined, (error) => {console.error(error)})
  }

  objSel.addEventListener('change', (e) => {samSel.value = ''})
  samSel.addEventListener('change', load_model2)
  load_model2()

  // resize renderers
  function resizeRenderers() {
    let content_width = document.querySelector('#results').offsetWidth
    renderer1.setSize(content_width, content_width / 1.618)
    renderer2.setSize(content_width, content_width / 1.618)
  }
  window.addEventListener('resize', () => {
    resizeRenderers()
  })
  resizeRenderers()
</script>

</html>